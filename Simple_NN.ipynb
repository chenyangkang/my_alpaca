{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr\n",
    "import wandb\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import alpaca\n",
    "from alpaca_trade_api import REST\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "\n",
    "### get stock data\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from alpaca.data.historical import StockHistoricalDataClient\n",
    "\n",
    "import sys\n",
    "sys.path = sys.path + ['../']\n",
    "\n",
    "import my_secrets\n",
    "import pickle\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = datetime.datetime.strptime('2018-01-01-00:00:00','%Y-%m-%d-%H:%M:%S')\n",
    "date_end = datetime.datetime.strptime('2023-04-02-00:00:00','%Y-%m-%d-%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_code='TQQQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# if os.path.exists(f'./historical_price_data/{stock_code}.csv'):\n",
    "#     continue\n",
    "request_params = StockBarsRequest(\n",
    "    symbol_or_symbols=stock_code,\n",
    "    timeframe=TimeFrame.Minute,\n",
    "    start=date_start,\n",
    "    end=date_end, \n",
    "    adjustment='all'\n",
    ")\n",
    "\n",
    "client = StockHistoricalDataClient(api_key=my_secrets.API_KEY, secret_key=my_secrets.SECRET_KEY) ### request stock price\n",
    "bars = client.get_stock_bars(request_params)\n",
    "bas_df = bars.df.droplevel(axis=0,level=0)\n",
    "bas_df.index = [i.replace(tzinfo=None).tz_localize(pytz.timezone('UTC')).tz_convert('US/Eastern') for i in bas_df.index]\n",
    "bas_df.to_csv(f'./historical_price_data/{stock_code}.csv',index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path = f'./historical_price_data/{stock_code}.csv'\n",
    "data = pd.read_csv(path,index_col=0)\n",
    "data.index = pd.Series([i.replace(tzinfo=None) for i in pd.to_datetime(data.index)]).dt.tz_localize(pytz.timezone('US/Eastern'))\n",
    "\n",
    "a = data.resample('1min').mean().dropna()\n",
    "a['change_rate'] = (a['close'] - a['open'])/a['open']\n",
    "a['change_bi'] = np.where((a['close'] - a['open'])>0,1,0)\n",
    "a['date'] = pd.to_datetime([i.date() for i in a.index])\n",
    "a['year'] = a.date.dt.year\n",
    "a['WOY'] = a.date.dt.week\n",
    "a['WOY_year'] = [str(a)+str(b) for a,b in zip(a['WOY'],a['year'])]\n",
    "a['MOY'] = a.date.dt.month\n",
    "a['MOY_year'] = [str(a)+str(b) for a,b in zip(a['MOY'],a['year'])]\n",
    "a['DOW'] = a.date.dt.day_of_week\n",
    "a['DOY'] = a.date.dt.day_of_year\n",
    "a['DOY_year'] = [str(a)+str(b) for a,b in zip(a['DOY'],a['year'])]\n",
    "a['MOD'] = [i.hour*60+i.minute for i in a.index]\n",
    "a['HOD'] = [i.hour for i in a.index]\n",
    "a = a[(a.MOD>=9.5*60) & (a.MOD<=16*60)]\n",
    "def calc_one_day(df):\n",
    "    start = df.iloc[0,:]['close']\n",
    "    df['change_from_today_start'] = (df['close'] - start)/start\n",
    "    return df\n",
    "a = a.groupby('date').apply(calc_one_day)\n",
    "a.groupby('MOD').mean()['change_from_today_start'].plot()\n",
    "#     plt.title(code)\n",
    "#     plt.show()\n",
    "a['code'] = stock_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.merge(\n",
    "    a[['DOY_year','change_rate']].groupby('DOY_year').sum().reset_index(drop=False).rename(columns={'change_rate':'change_rate_DOY_year'}),\n",
    "    left_on='DOY_year', right_on='DOY_year',how='left'\n",
    ").merge(\n",
    "    a[['WOY_year','change_rate']].groupby('WOY_year').sum().reset_index(drop=False).rename(columns={'change_rate':'change_rate_WOY_year'}),\n",
    "    left_on='WOY_year', right_on='WOY_year',how='left'\n",
    ").merge(\n",
    "    a[['MOY_year','change_rate']].groupby('MOY_year').sum().reset_index(drop=False).rename(columns={'change_rate':'change_rate_MOY_year'}),\n",
    "    left_on='MOY_year', right_on='MOY_year',how='left'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142350.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "((6.5*60)/1) * 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### predict based on past 15 days. 5min interval, 2h+30min+4h = 6.5h. ((6.5*60)/1) * 15 = 5850 block\n",
    "# each interval being predicted using the past 1170 block\n",
    "\n",
    "### predict based on past 15 weeks. 5min interval, 2h+30min+4h = 6.5h. ((6.5*60)/1) * 15 * 7 = 40950 block\n",
    "\n",
    "### predict based on past 15 month. 5min interval, 2h+30min+4h = 6.5h. ((6.5*60)/1) * 15 * 30= 175500 block\n",
    "\n",
    "feature_dim_minute = 5850\n",
    "feature_dim_day = 30\n",
    "feature_dim_week = 15\n",
    "feature_dim_month = 15\n",
    "\n",
    "mi_value = [i for i in a['change_rate'].values]\n",
    "day_value = [i for i in a['change_rate_DOY_year'].values]\n",
    "uniq_day_value = list(a['change_rate_DOY_year'].unique())\n",
    "week_value = [i for i in a['change_rate_WOY_year'].values]\n",
    "uniq_week_value = list(a['change_rate_WOY_year'].unique())\n",
    "month_value = [i for i in a['change_rate_MOY_year'].values]\n",
    "uniq_month_value = list(a['change_rate_MOY_year'].unique())\n",
    "\n",
    "df = []\n",
    "batch_count = 0\n",
    "for index,v in tqdm(enumerate(zip(mi_value,day_value,week_value,month_value)), total=len(mi_value)):\n",
    "    if index<feature_dim_month*(6.5*60)*22+10:\n",
    "        continue\n",
    "    \n",
    "    ### month level, 15 month\n",
    "    this_month_index = uniq_month_value.index(month_value[index])\n",
    "    month_list = uniq_month_value[int(this_month_index-feature_dim_month):this_month_index][::-1] ### arround 25 days per month for trading\n",
    "    \n",
    "    ### week level, 15 week\n",
    "    this_week_index = uniq_week_value.index(week_value[index])\n",
    "    week_list = uniq_week_value[int(this_week_index-feature_dim_week):this_week_index][::-1] ### arround 5 days per week for trading\n",
    "    \n",
    "    ### day level, 30 days\n",
    "    this_day_index = uniq_day_value.index(day_value[index])\n",
    "    day_list = uniq_day_value[int(this_day_index-feature_dim_day):this_day_index][::-1]\n",
    "\n",
    "    ### minitue level, 5850 minutes\n",
    "    minute_list = mi_value[int(index-feature_dim_minute):index][::-1]\n",
    "    \n",
    "    df.append({\n",
    "        **{'y':v[0]},\n",
    "        **{f'm{i}':j for i,j in zip(list(range(feature_dim_month)),month_list)},\n",
    "        **{f'w{i}':j for i,j in zip(list(range(feature_dim_week)),week_list)},\n",
    "        **{f'd{i}':j for i,j in zip(list(range(feature_dim_day)),day_list)},\n",
    "        **{f'mi{i}':j for i,j in zip(list(range(feature_dim_minute)),minute_list)}\n",
    "    })\n",
    "    \n",
    "    if len(df)>=5120:\n",
    "        df = pd.DataFrame(df)\n",
    "        with open(f'./training_data/{stock_code}_processed_data_{batch_count}.pkl','wb') as f:\n",
    "            pickle.dump(df,f)\n",
    "        batch_count+=1\n",
    "        df=[]\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>m0</th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "      <th>m3</th>\n",
       "      <th>m4</th>\n",
       "      <th>m5</th>\n",
       "      <th>m6</th>\n",
       "      <th>m7</th>\n",
       "      <th>m8</th>\n",
       "      <th>...</th>\n",
       "      <th>mi5840</th>\n",
       "      <th>mi5841</th>\n",
       "      <th>mi5842</th>\n",
       "      <th>mi5843</th>\n",
       "      <th>mi5844</th>\n",
       "      <th>mi5845</th>\n",
       "      <th>mi5846</th>\n",
       "      <th>mi5847</th>\n",
       "      <th>mi5848</th>\n",
       "      <th>mi5849</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.124534</td>\n",
       "      <td>-0.350429</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.398792</td>\n",
       "      <td>-0.084649</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>-0.000665</td>\n",
       "      <td>-0.001328</td>\n",
       "      <td>-0.001326</td>\n",
       "      <td>-0.000663</td>\n",
       "      <td>-0.000663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001229</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.124534</td>\n",
       "      <td>-0.350429</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.398792</td>\n",
       "      <td>-0.084649</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>-0.000665</td>\n",
       "      <td>-0.001328</td>\n",
       "      <td>-0.001326</td>\n",
       "      <td>-0.000663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001231</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.124534</td>\n",
       "      <td>-0.350429</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.398792</td>\n",
       "      <td>-0.084649</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001332</td>\n",
       "      <td>-0.001995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>-0.000665</td>\n",
       "      <td>-0.001328</td>\n",
       "      <td>-0.001326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002465</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.124534</td>\n",
       "      <td>-0.350429</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.398792</td>\n",
       "      <td>-0.084649</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001334</td>\n",
       "      <td>-0.001332</td>\n",
       "      <td>-0.001995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>-0.000665</td>\n",
       "      <td>-0.001328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000618</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.124534</td>\n",
       "      <td>-0.350429</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.398792</td>\n",
       "      <td>-0.084649</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>-0.001334</td>\n",
       "      <td>-0.001332</td>\n",
       "      <td>-0.001995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>-0.000665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.124534</td>\n",
       "      <td>-0.350429</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.398792</td>\n",
       "      <td>-0.084649</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.001205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>0.003541</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.124534</td>\n",
       "      <td>-0.350429</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.398792</td>\n",
       "      <td>-0.084649</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.001205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>0.002117</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.124534</td>\n",
       "      <td>-0.350429</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.398792</td>\n",
       "      <td>-0.084649</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.000603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>0.000704</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.124534</td>\n",
       "      <td>-0.350429</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.398792</td>\n",
       "      <td>-0.084649</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>0.000604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>0.000703</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.018896</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.124534</td>\n",
       "      <td>-0.350429</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.398792</td>\n",
       "      <td>-0.084649</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>-0.000603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5120 rows Ã— 5911 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             y        m0        m1        m2        m3        m4        m5  \\\n",
       "0     0.000000  0.018896  0.068262  0.124534 -0.350429 -0.026531 -0.398792   \n",
       "1    -0.001229  0.018896  0.068262  0.124534 -0.350429 -0.026531 -0.398792   \n",
       "2    -0.001231  0.018896  0.068262  0.124534 -0.350429 -0.026531 -0.398792   \n",
       "3    -0.002465  0.018896  0.068262  0.124534 -0.350429 -0.026531 -0.398792   \n",
       "4    -0.000618  0.018896  0.068262  0.124534 -0.350429 -0.026531 -0.398792   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5115  0.000000 -0.013343  0.018896  0.068262  0.124534 -0.350429 -0.026531   \n",
       "5116  0.003541 -0.013343  0.018896  0.068262  0.124534 -0.350429 -0.026531   \n",
       "5117  0.002117 -0.013343  0.018896  0.068262  0.124534 -0.350429 -0.026531   \n",
       "5118  0.000704 -0.013343  0.018896  0.068262  0.124534 -0.350429 -0.026531   \n",
       "5119  0.000703 -0.013343  0.018896  0.068262  0.124534 -0.350429 -0.026531   \n",
       "\n",
       "            m6        m7        m8  ...    mi5840    mi5841    mi5842  \\\n",
       "0    -0.084649  0.050870 -0.011415  ...  0.000000 -0.001329  0.000000   \n",
       "1    -0.084649  0.050870 -0.011415  ... -0.001995  0.000000 -0.001329   \n",
       "2    -0.084649  0.050870 -0.011415  ... -0.001332 -0.001995  0.000000   \n",
       "3    -0.084649  0.050870 -0.011415  ... -0.001334 -0.001332 -0.001995   \n",
       "4    -0.084649  0.050870 -0.011415  ...  0.000668 -0.001334 -0.001332   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5115 -0.398792 -0.084649  0.050870  ...  0.000000  0.000000  0.000605   \n",
       "5116 -0.398792 -0.084649  0.050870  ...  0.000000  0.000000  0.000000   \n",
       "5117 -0.398792 -0.084649  0.050870  ...  0.000605  0.000000  0.000000   \n",
       "5118 -0.398792 -0.084649  0.050870  ...  0.000604  0.000605  0.000000   \n",
       "5119 -0.398792 -0.084649  0.050870  ... -0.001208  0.000604  0.000605   \n",
       "\n",
       "        mi5843    mi5844    mi5845    mi5846    mi5847    mi5848    mi5849  \n",
       "0     0.000000  0.001331 -0.000665 -0.001328 -0.001326 -0.000663 -0.000663  \n",
       "1     0.000000  0.000000  0.001331 -0.000665 -0.001328 -0.001326 -0.000663  \n",
       "2    -0.001329  0.000000  0.000000  0.001331 -0.000665 -0.001328 -0.001326  \n",
       "3     0.000000 -0.001329  0.000000  0.000000  0.001331 -0.000665 -0.001328  \n",
       "4    -0.001995  0.000000 -0.001329  0.000000  0.000000  0.001331 -0.000665  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5115 -0.001209 -0.001208 -0.000603  0.000604 -0.000603 -0.001205  0.000000  \n",
       "5116  0.000605 -0.001209 -0.001208 -0.000603  0.000604 -0.000603 -0.001205  \n",
       "5117  0.000000  0.000605 -0.001209 -0.001208 -0.000603  0.000604 -0.000603  \n",
       "5118  0.000000  0.000000  0.000605 -0.001209 -0.001208 -0.000603  0.000604  \n",
       "5119  0.000000  0.000000  0.000000  0.000605 -0.001209 -0.001208 -0.000603  \n",
       "\n",
       "[5120 rows x 5911 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51544.4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "515444/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('TQQQ_processed.pkl','wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### some hyperparameters\n",
    "MIN_DELTA = 0.001\n",
    "MIN_LR = 0.0001\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define data loader\n",
    "class LargeDataset(Dataset):\n",
    "    def __init__(self, data_dir, stock_code, block_index_list):\n",
    "        self.stock_code = stock_code\n",
    "        self.data_dir = data_dir\n",
    "        self.file_list = [i for i in sorted(os.listdir(data_dir)) if stock_code in i and int(i.split('.pkl')[0].split('_')[-1]) in block_index_list]  # List all files in the data directory\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        x, y = np.expand_dims(data.iloc[:,1:].values, -1), data.iloc[:,0].values  # Assuming the data is stored as dictionaries with 'x' and 'y' keys\n",
    "        return x, y\n",
    "\n",
    "### define data loader\n",
    "class subDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.x[idx,:], self.y[idx]  # Assuming the data is stored as dictionaries with 'x' and 'y' keys\n",
    "        return x, y\n",
    "    \n",
    "class myloader():\n",
    "    def __init__(self, data_dir, stock_code, block_index_list):\n",
    "        self.stock_code = stock_code\n",
    "        self.block_index_list = block_index_list\n",
    "        self.overall_train_dataset = LargeDataset(data_dir,stock_code,block_index_list)\n",
    "        self.current_sub_dataset = None\n",
    "    \n",
    "    def get_data(self):\n",
    "        for block_idx in range(len(self.block_index_list)):\n",
    "            X_block, y_block = self.overall_train_dataset[block_idx]\n",
    "            sub_dataloader = DataLoader(\n",
    "                    subDataset(X_block, y_block),\n",
    "                    batch_size=1024,\n",
    "                    shuffle=False\n",
    "                )\n",
    "            for x,y in sub_dataloader:\n",
    "                yield x,y\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define some utils\n",
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after patience epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=MIN_DELTA):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            # print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                # print('INFO: Early stopping')\n",
    "                self.early_stop = True\n",
    "                \n",
    "                \n",
    "class LRScheduler():\n",
    "    \"\"\"\n",
    "    Learning rate scheduler. If the validation loss does not decrease for the \n",
    "    given number of `patience` epochs, then the learning rate will decrease by\n",
    "    by given `factor`.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, patience=3, min_lr=MIN_LR, factor=0.1):\n",
    "        \"\"\"\n",
    "        new_lr = old_lr * factor\n",
    "        :param optimizer: the optimizer we are using\n",
    "        :param patience: how many epochs to wait before updating the lr\n",
    "        :param min_lr: least lr value to reduce to while updating\n",
    "        :param factor: factor by which the lr should be updated\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.factor = factor\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "                self.optimizer,\n",
    "                mode='min',\n",
    "                patience=self.patience,\n",
    "                factor=self.factor,\n",
    "                min_lr=self.min_lr,\n",
    "                verbose=True\n",
    "            )\n",
    "    def __call__(self, val_loss):\n",
    "        # take one step of the learning rate scheduler while providing the validation loss as the argument\n",
    "        self.lr_scheduler.step(val_loss)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define the model\n",
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_size = hidden_size # number of features in hidden state\n",
    "        self.num_layers = num_layers #number of lstm layers\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 8, kernel_size=64) # in_channels, out_channels, kernel_size\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=4)\n",
    "        self.conv2 = nn.Conv1d(8, 16,kernel_size=32)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=4)        \n",
    "        self.conv3 = nn.Conv1d(16, 32,kernel_size=16)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        \n",
    "        self.lstm_month = nn.LSTM(input_size=15, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, bidirectional=False) #lstm\n",
    "        self.lstm_week = nn.LSTM(input_size=15, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, bidirectional=False) #lstm\n",
    "        self.lstm_day = nn.LSTM(input_size=30, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, bidirectional=False) #lstm\n",
    "        self.lstm_minute = nn.LSTM(input_size=84, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, bidirectional=False) #lstm\n",
    "\n",
    "        self.fc_1 =  nn.Linear(266,256) #fully connected 1\n",
    "        self.fc_2 =  nn.Linear(256,32) #fully connected 2\n",
    "        self.fc_3 =  nn.Linear(32,1) #fully connected 3\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc_1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc_2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc_3.weight)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        ##### LSTM for month data\n",
    "        month_data = x[:,:,0:15]\n",
    "        h_0_month = Variable(torch.zeros(self.num_layers, month_data.size(0), self.hidden_size)).to(self.device)\n",
    "        c_0_month = Variable(torch.zeros(self.num_layers, month_data.size(0), self.hidden_size)).to(self.device)\n",
    "        output_month, (hn_month, cn_month) = self.lstm_month(month_data, (h_0_month, c_0_month)) # hn.shape = 1, N, 64\n",
    "        hn_month = hn_month[hn_month.shape[0]-1:]\n",
    "        hn_month = hn_month.view(-1, self.hidden_size) # hn.shape = N, 64\n",
    "        \n",
    "        ##### LSTM for week data\n",
    "        week_data = x[:,:,15:30]\n",
    "        h_0_week = Variable(torch.zeros(self.num_layers, week_data.size(0), self.hidden_size)).to(self.device)\n",
    "        c_0_week = Variable(torch.zeros(self.num_layers, week_data.size(0), self.hidden_size)).to(self.device)\n",
    "        output_week, (hn_week, cn_week) = self.lstm_month(week_data, (h_0_week, c_0_week)) # hn.shape = 1, N, 64\n",
    "        hn_week = hn_week[hn_week.shape[0]-1:]\n",
    "        hn_week = hn_month.view(-1, self.hidden_size) # hn.shape = N, 64\n",
    "        \n",
    "        ##### LSTM for day data\n",
    "        day_data = x[:,:,30:60]\n",
    "        h_0_day = Variable(torch.zeros(self.num_layers, day_data.size(0), self.hidden_size)).to(self.device)\n",
    "        c_0_day = Variable(torch.zeros(self.num_layers, day_data.size(0), self.hidden_size)).to(self.device)\n",
    "        output_day, (hn_day, cn_day) = self.lstm_day(day_data, (h_0_day, c_0_day)) # hn.shape = 1, N, 64\n",
    "        hn_day = hn_day[hn_day.shape[0]-1:]\n",
    "        hn_day = hn_day.view(-1, self.hidden_size) # hn.shape = N, 64\n",
    "        \n",
    "        \n",
    "        ##### convolution for minute data\n",
    "        # print(x.shape) # N, 1, 24\n",
    "        out  = self.conv1(x[:,:,60:]) # default activation is None; out.shape: N, 8, 24\n",
    "        out = self.pool1(out) # out.shape: N, 8, 12\n",
    "        \n",
    "        out = self.conv2(out) # out.shape: N, 16, 12\n",
    "        out = self.pool2(out) # out.shape: N, 16, 6\n",
    "        \n",
    "        out = self.conv3(out) # out.shape: N, 32, 6\n",
    "        out = self.pool3(out) # out.shape: N, 32, 3\n",
    "        \n",
    "        h_0 = Variable(torch.zeros(self.num_layers, out.size(0), self.hidden_size)).to(self.device)\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, out.size(0), self.hidden_size)).to(self.device)\n",
    "\n",
    "        output, (hn, cn) = self.lstm_minute(out, (h_0, c_0)) # hn.shape = 1, N, 64\n",
    "        hn = hn[hn.shape[0]-1]\n",
    "        hn = hn.view(-1, self.hidden_size) # hn.shape = N, 64\n",
    "        \n",
    "        input_to_fcl = torch.cat((hn_month, hn_week, hn_day, hn, x[:,0,-10:]), axis=1)\n",
    "        out = self.fc_1(input_to_fcl) # Dense, out.shape = N, 256\n",
    "        out = self.fc_2(out) # out.shape = N, 32\n",
    "        out = self.fc_3(out) # out.shape = N, 12\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load data\n",
    "train_loader = myloader('./training_data/','TQQQ',list(range(0,65)))\n",
    "val_loader = myloader('./training_data/','TQQQ',list(range(65,75)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lstm_model = LSTM_model(HIDDEN_SIZE, NUM_LAYERS)\n",
    "lstm_model = lstm_model.to(device).float()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = LRScheduler(optimizer)\n",
    "early_stopping = EarlyStopping()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/325 [00:19<52:23,  9.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m### back prop\u001b[39;00m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### start training\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "min_val_loss = np.inf\n",
    "\n",
    "\n",
    "# # logging\n",
    "# wandb.init(\n",
    "#     project=\"TQQQ_prediciton_pure_hist_price\",\n",
    "#     config={\n",
    "#     \"learning_rate\": LEARNING_RATE,\n",
    "#     \"architecture\": \"LSTM\",\n",
    "#     \"dataset\": \"TQQQ\",\n",
    "#     \"epochs\": NUM_EPOCHS,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    ## train\n",
    "    lstm_model.train()\n",
    "    for inputs, targets in tqdm(train_loader.get_data(),total=len(train_loader.block_index_list)*5):\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.float()\n",
    "        inputs = inputs.permute([0,2,1])\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        ### train, pred\n",
    "        preds = lstm_model(inputs)\n",
    "        loss = criterion(preds, targets)\n",
    "        \n",
    "        ### back prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            train_losses.append(loss.item())\n",
    "    \n",
    "    ### eval\n",
    "    r2_list = []\n",
    "    spearmanr_list = []\n",
    "    lstm_model.eval()\n",
    "    for inputs, targets in val_loader.get_data():\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.float()\n",
    "        inputs = inputs.permute([0,2,1])\n",
    "        inputs = inputs.to(device)\n",
    "        preds = lstm_model(inputs)\n",
    "        val_loss = criterion(preds, targets)\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        ### r2 and spearman_r\n",
    "        r2 = r2_score(targets.detach().numpy().flatten(),\n",
    "                 preds.detach().numpy().flatten()\n",
    "                )\n",
    "        spearman_r = spearmanr(targets.detach().numpy().flatten(),\n",
    "                 preds.detach().numpy().flatten()\n",
    "                )[0]\n",
    "        r2_list.append(r2)\n",
    "        spearmanr_list.append(spearman_r)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_epoch_loss = np.mean(val_losses)\n",
    "        train_epoch_loss = np.mean(train_losses)\n",
    "        if epoch%1 == 0 or epoch==(NUM_EPOCHS-1):\n",
    "            print(\"Epoch: {}/{}...\".format(epoch, NUM_EPOCHS),\n",
    "                  \"Loss: {:.6f}...\".format(train_epoch_loss),\n",
    "                  \"Val Loss: {:.6f}\".format(val_epoch_loss))\n",
    "\n",
    "        if val_epoch_loss < min_val_loss:\n",
    "            min_val_loss = val_epoch_loss\n",
    "#             best_hidden_size = hidden_size\n",
    "            best_model = lstm_model\n",
    "\n",
    "        lr_scheduler(val_epoch_loss)\n",
    "        early_stopping(val_epoch_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping after epoch: {}/{}...\".format(epoch, NUM_EPOCHS),\n",
    "                  \"Loss: {:.6f}...\".format(np.mean(train_losses)),\n",
    "                  \"Val Loss: {:.6f}\".format(val_epoch_loss))            \n",
    "            break\n",
    "            \n",
    "    for param_group in lr_scheduler.optimizer.param_groups:\n",
    "        this_lr = param_group['lr']\n",
    "    wandb.log({\"train_loss\": train_epoch_loss, \"val_loss\": val_epoch_loss, \n",
    "               'val_r2':np.nanmean(r2_list), 'val_spearmanr':np.nanmean(spearmanr_list), 'lr':this_lr})\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/325 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(train_loader.get_data(),total=len(train_loader.block_index_list)*5):\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.float()\n",
    "        inputs = inputs.permute([0,2,1])\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        ### train, pred\n",
    "        preds = lstm_model(inputs)\n",
    "        loss = criterion(preds, targets)\n",
    "        \n",
    "        ### back prop\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearman_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=nan, pvalue=nan)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(targets.detach().numpy(),\n",
    "         preds.detach().numpy().flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.036112137138843536,\n",
       " 1427162.75,\n",
       " 12306202.0,\n",
       " 5488707072.0,\n",
       " 25637078.0,\n",
       " 17547354.0,\n",
       " 12477466.0,\n",
       " 9092305.0,\n",
       " 6736295.5,\n",
       " 5048871.5,\n",
       " 3815373.0,\n",
       " 2900171.0,\n",
       " 2213623.75,\n",
       " 1694489.5,\n",
       " 1299653.0,\n",
       " 997935.5625,\n",
       " 766811.9375,\n",
       " 589319.125,\n",
       " 452861.4375,\n",
       " 347869.0,\n",
       " 267043.90625,\n",
       " 204845.171875,\n",
       " 156990.25,\n",
       " 120195.78125,\n",
       " 91918.421875,\n",
       " 70216.9375,\n",
       " 53570.51953125,\n",
       " 40816.1640625,\n",
       " 31059.31640625,\n",
       " 23603.078125,\n",
       " 17912.25390625,\n",
       " 13572.0078125,\n",
       " 10270.884765625,\n",
       " 7760.15673828125,\n",
       " 5854.61865234375,\n",
       " 4410.3740234375,\n",
       " 3318.45654296875,\n",
       " 2492.680419921875,\n",
       " 1869.0374755859375,\n",
       " 1399.6297607421875,\n",
       " 1046.38134765625,\n",
       " 781.0643310546875,\n",
       " 582.0787353515625,\n",
       " 433.144287109375,\n",
       " 321.8984680175781,\n",
       " 238.70437622070312,\n",
       " 176.73374938964844,\n",
       " 130.71511840820312,\n",
       " 96.43936157226562,\n",
       " 71.23314666748047,\n",
       " 52.33595275878906,\n",
       " 38.38673400878906,\n",
       " 28.20049285888672,\n",
       " 20.608150482177734,\n",
       " 15.104609489440918,\n",
       " 10.995914459228516,\n",
       " 8.032283782958984,\n",
       " 5.81132173538208,\n",
       " 4.229701042175293,\n",
       " 3.0629982948303223,\n",
       " 2.215820550918579,\n",
       " 1.5991768836975098,\n",
       " 1.1584182977676392,\n",
       " 0.832249104976654,\n",
       " 0.6103571057319641,\n",
       " 0.44647732377052307,\n",
       " 0.3151627779006958,\n",
       " 0.24819965660572052,\n",
       " 0.16512630879878998,\n",
       " 0.13904084265232086,\n",
       " 0.1068800836801529,\n",
       " 0.08733081072568893,\n",
       " 0.05522633343935013,\n",
       " 0.050243280827999115,\n",
       " 0.024362683296203613,\n",
       " 0.01876017637550831,\n",
       " 0.01824442483484745,\n",
       " 0.01876743510365486,\n",
       " 0.010960870422422886,\n",
       " 0.011858411133289337,\n",
       " 0.014875016175210476,\n",
       " 0.018000811338424683,\n",
       " 0.010926716029644012,\n",
       " 0.00926226656883955,\n",
       " 0.02162369340658188,\n",
       " 0.028091061860322952,\n",
       " 0.033278677612543106,\n",
       " 0.0702221691608429,\n",
       " 0.015367446467280388,\n",
       " 0.018513021990656853,\n",
       " 0.042206715792417526,\n",
       " 0.05538267642259598,\n",
       " 0.012967283837497234,\n",
       " 0.01238244865089655,\n",
       " 0.0070465197786688805,\n",
       " 0.020397095009684563,\n",
       " 0.04882447421550751,\n",
       " 0.04047157242894173,\n",
       " 0.045552924275398254,\n",
       " 0.04077565670013428,\n",
       " 0.17934808135032654,\n",
       " 0.13383379578590393,\n",
       " 0.05190320685505867,\n",
       " 0.05772365257143974,\n",
       " 0.034568704664707184,\n",
       " 0.05404356122016907,\n",
       " 0.05119561403989792,\n",
       " 0.03874482959508896,\n",
       " 0.08827894181013107,\n",
       " 0.07432401180267334,\n",
       " 0.05400824174284935,\n",
       " 0.09407074004411697,\n",
       " 0.07179611921310425,\n",
       " 0.048148684203624725,\n",
       " 0.05483545735478401,\n",
       " 0.03323841094970703,\n",
       " 0.031275976449251175,\n",
       " 0.02307083271443844,\n",
       " 0.023225344717502594,\n",
       " 0.022532159462571144,\n",
       " 0.050435084849596024,\n",
       " 0.032490864396095276,\n",
       " 0.03044929914176464,\n",
       " 0.03618617355823517,\n",
       " 0.08791161328554153,\n",
       " 0.08995120227336884,\n",
       " 0.09254554659128189,\n",
       " 0.1668654829263687,\n",
       " 0.13716867566108704,\n",
       " 0.1779586374759674,\n",
       " 0.0708172544836998,\n",
       " 0.09949441999197006,\n",
       " 0.09286627173423767,\n",
       " 0.04018520191311836,\n",
       " 0.061500828713178635,\n",
       " 0.05101031810045242,\n",
       " 0.03396095708012581,\n",
       " 0.08596523106098175,\n",
       " 0.11104196310043335,\n",
       " 0.04950403422117233,\n",
       " 0.049695201218128204,\n",
       " 0.054688990116119385,\n",
       " 0.07285519689321518,\n",
       " 0.04680868238210678,\n",
       " 0.03419045731425285,\n",
       " 0.07608085870742798,\n",
       " 0.0235440731048584,\n",
       " 0.0417068675160408,\n",
       " 0.027595769613981247,\n",
       " 0.03366275504231453,\n",
       " 0.03802387788891792,\n",
       " 0.031605008989572525,\n",
       " 0.025003595277667046,\n",
       " 0.022511975839734077,\n",
       " 0.024913832545280457,\n",
       " 0.017427958548069,\n",
       " 0.022172510623931885,\n",
       " 0.05058656632900238,\n",
       " 0.05064688250422478,\n",
       " 0.061693448573350906,\n",
       " 0.034161441028118134,\n",
       " 0.03726722300052643,\n",
       " 0.045491646975278854,\n",
       " 0.028862176463007927,\n",
       " 0.10368819534778595,\n",
       " 0.05817205831408501,\n",
       " 0.05831707641482353,\n",
       " 0.03558351472020149,\n",
       " 0.0452672578394413,\n",
       " 0.10614868998527527,\n",
       " 0.07177269458770752,\n",
       " 0.06798295676708221]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e9867b585c0f10e2eb480253e40cab44b53d9f15cdd7fb9c79b17a5cb2fa039"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
